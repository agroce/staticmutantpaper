\section{Experimental Results: Comparison and Improvement of Static Analysis Tools}

Our primary experimental results are a set of comparisons of tools using our method, for three languages: Solidity (the most popular language for smart contracts), Java, and Python.  We also use these results to answer a set of research questions that consider the utility of our method:

\begin{itemize}
\item {\bf RQ1:}  Does mutation analysis of static analysis tools produce actionable results?  That is, do raw mutation kills serve to distinguish tools from each other, or are all tools similar in terms of mutation detection capability?
\item {\bf RQ2:}  Does our approach provide additional information beyond simply counting findings for the original, un-mutated analyzed code?  Do \emph{ratios} differ between programs, or does the number of mutants killed simply reflect the ``verbosity'' of each tool?
\item {\bf RQ3:}  Do the rankings that raw kills and ratios establish agree with other sources of information about the effectiveness of the evaluated tools?  Can we confirm results from other evaluation methods, or informal opinion?
\item {\bf RQ4:}  Do mutation scores for clean programs differ from those for programs for which tools report findings before mutation?
\item {\bf RQ5:}  Do individual mutants, prioritized for ease of examination, allow us to identify classes of faults that
 different tools are good at/bad at?  Can we extend this information to improve tools by addressing identified weaknesses?
\item {\bf RQ6:}  How easy is it identify low-hanging fruit for improving tools without prioritization to remove conceptually redundant mutants?
  \end{itemize}

In particular, we consider {\bf RQ2} to be of critical importance; if the mutant ratios for tools differ, then this is clear evidence that our hypothesis that the tendency of mutants to be faults, and to expect that mutated code will, by a sound and precise tool, be flagged as problematic more often than non-mutated code, holds.  This expectation that (some subset of the) mutants can serve as proxies for real, detectable faults is the core concept of our approach.
{\bf RQ4} addresses a concern briefly mentioned in the introduction:  it is possible that warnings for the original code interfere with our definition of detection.  The ideal case for our approach is when a tool report no findings for un-mutated code, and either reports a finding (kills the mutant) or not (does not kill it) when the mutant is introduced.  Chekam et al. showed that the ``clean program assumption'' for testing, the idea that coverage will be similar for faulty and fixed versions of a program, is a threat to the validity of investigations of the relationship between coverage and fault detection \cite{CleanProgram}, and we want to establish that this is unlikely to be the case for our approach.
{\bf RQ5} and {\bf RQ6} receive only preliminary answers, for the smart contract tools.

\subsection{Solidity Smart Contract Tools}

\subsubsection{Smart Contracts and Smart Contract Static Analysis}

Smart contracts are autonomous code instruments, usually operating on a blockchain, that often have critical responsibilities such as facilitating and verifying (large) financial services transactions, tracking high-value physical goods or intellectual property, or even controlling ``decentralized organizations'' with multifarious aspects.  Security and correctness are thus critical in the smart contract domain, and static analysis is a key way to ensure allocation of high-value resources is not compromised.  The most popular smart contract platform, by far, is the Ethereum blockchain, and the Solidity smart contract language \cite{buterin2013whitepaper,wood2014yellow}; the Ethereum cryptocurrency has a market capitalization as we write of over \$15 billion dollars, largely fueled by interest in the smart contract functionality.  Ethereum contracts have been the targets of widely publicized attacks, with large financial consequences  \cite{spank,DAO}.   A recent paper examining results from 23 professional security audits of Solidity contracts argues that effective static analysis is a major key to avoiding such disasters in the future \cite{FC20}.

\subsubsection{Static Analysis Tools Compared}

We analyzed three well-known tools for static analysis of Solidity smart contracts: Slither \cite{slither}, SmartCheck \cite{smartcheck}, and Securify \cite{Securify}.  Slither, based on an SSA-based intermediate language (SlithIR \cite{slither}) is a tool produced by Trail of Bits.  SmartCheck, developed by SmartDec, translates Solidity source directly to an XML-based representation, then uses \emph{XPath} patterns to define problems.  Securify, from SRI Systems Lab at ETH Zurich, works at the bytecode level, first parsing and decompiling contracts, then translating to \emph{semantic facts} in order to look for predefined problems.

\subsubsection{Smart Contract Selection}

We could have used a set of high-transaction contracts, or known-important contracts to validate our approach.  However, we knew that one of our goals in the Solidity experiments was to actually improve a mutation analysis tool, and the developers of the static analysis tools use exactly such benchmarks to validate their tools.  Basing our improvements on mutants of the contracts used for evaluation of proposed detectors would introduce a serious bias in our favor: we would be more likely to produce detectors that would have true positives and few false positives on the benchmark contracts.  We therefore instead selected 100 random contracts for which EtherScan (\url{https://etherscan.io/}) has source code, and used this (quite arbitrary) set of contracts from the actual blockchain to compare tools and identify opportunities for improvement.  The collected contracts had a total of 15,980 non-comment source lines, as measured by {\tt cloc}, with a mean size of 159.8 LOC and a median size of 108 LOC.  The largest single contract had 1,127 lines of code.  The Universal Mutator generated 46,769 valid mutants for these 100 contracts.

\subsubsection{Analysis Results}

\begin{figure}
  \includegraphics[width=0.8\columnwidth]{solidity.png}
  \caption{Mutants killed by Solidity static analysis tools.}
  \label{fig:solidityvenn}
\end{figure}

\begin{table}
  \begin{tabular}{l|r|r|r|r|r}
    & \multicolumn{2}{|c|}{Findings} & \multicolumn{2}{|c|}{Mutation Score}  & Mutant \\
    Tool & Mean & Median & Mean & Median & Ratio\\
    \hline
    \hline
    Slither & 2.37 & 1.0 & 0.09 & 0.09 & 0.038 \\
    \hline
    SmartCheck & 1.89 & 1.0 & 0.05 & 0.05 & 0.026 \\
    \hline
    Securify & 24.65 & 17.0 & 0.03 & 0.02 &  0.001 \\
    \hline
  \end{tabular}
  \caption{Solidity tools: number of findings and mutation scores over all contracts.}
  \label{tab:scoresolidity}
\end{table}

\begin{table}
  \begin{tabular}{l|r|r|r|r|r}
    & & \multicolumn{2}{|c|}{} & \multicolumn{2}{|c}{Clean For All (3)} \\
    & \# Clean & \multicolumn{2}{|c|}{Mutation Score} &  \multicolumn{2}{|c}{Mutation Score}\\
    Tool & Contracts & Mean & Median & Mean & Median\\
    \hline
    \hline
    Slither & 39 & 0.11 & 0.11 & 0.09 & 0.09 \\
    \hline
    SmartCheck & 27 & 0.03 & 0.01 & 0.03 & 0.00 \\
    \hline
    Securify & 5 & 0.00 & 0.00 & 0.00 & 0.00 \\
    \hline
  \end{tabular}
  \caption{Solidity tools: clean contract counts and mutation scores.}
  \label{tab:cleansolidity}
\end{table}


Figure \ref{fig:solidityvenn} shows the mutants killed by the Solidity analysis tools.  Tables \ref{tab:scoresolidity} and \ref{tab:cleansolidity} provide numeric details of the results, including the \emph{ratio} for each tool, adjusting its mutation scores by its' general tendency to produce findings.  First, a user examining these results would suspect that Slither and SmartCheck are both useful tools, and should likely both be applied in a high-risk security-sensitive context like smart contract development.  Second, a user might suspect that the large number of findings produced, and smaller number of mutants killed, for Securify, mean that including Securify in the static analysis tool stable is a more difficult decision.  On the one hand, Securify does detect nearly as many mutants it alone can identify as SmartCheck.  The large number of findings, and very bad mutant ratio, however, lead us to suspect that many of these ``detected'' mutants are false positives (or, at least, that the problem is not the one Securify identifies).  Extracting the signal from Securify's noise will be difficult.  We also note that while running Slither and SmartCheck on all 46,769 valid mutants was relatively quick (it took about 6 days sequential compute time for Slither and 3 days for SmartCheck), Securify often required many hours to analyze a mutant, and frequently required a few days to analyze a mutant; the full analysis required over three months of compute time.

For our research questions, {\bf RQ1} is clearly answered in the affirmative.  Figure \ref{fig:solidityvenn} shows that the tools address quite different problems, with all tools reporting far more uniquely detected mutants than mutants in common with other tools.  There are only 18 mutants detected by all tools, all of them involving replacement of {\tt msg.sender} (the caller of a smart contract, which may be another smart contract) with {\tt tx.origin} (the original initiator of a sequence of blockchain calls, a ``human'' account).  Use of {\tt tx.origin} is usually a bad idea, can lead to incorrect behavior, and may not be relied on to be meaningful at all in future versions of Ethereum (\url{https://ethereum.stackexchange.com/questions/196/how-do-i-make-my-dapp-serenity-proof}).  

{\bf RQ2} is also answered in the affirmative.  Counting findings for un-mutated code might suggest that Securify is the best tool, by a wide margin, but in the context of its near-zero mutant ratio, we must suspect that many of the warnings are false positives.  Slither has the best mutant ratio, but the margin between it and SmartCheck confirms that both tools likely provide value.

For {\bf RQ3}, there are only a few tool comparisons in the literature; this is probably due to the fast-moving nature of the blockchain analysis world; the oldest of these tools' publication dates is 2018.  The most extensive is that of Durieux et al. \cite{durieux2019empirical}.  Slither detected 17\% of known vulnerabilities in their analysis, vs. 11\% for SmartCheck and 9\% for Securify. Slither and SmartCheck were also among the four (out of 9) tools that detected vulnerabilities in the most categories; Securify was not.  The overall recommendation of Durieux et al. was to use a combination of Slither and Mythril \cite{mythril-code} for contract analysis.  Parizi et al. \cite{Parizi} also offer a ranking of tools, and determined that SmartCheck was the most effective, and far more so than Securify; unfortunately, they did not include Slither in their set of evaluated tools.

The Slither paper \cite{slither} also provides an evaluation of all three tools.  Their findings counts differ from ours because of different choices (we threw out merely informational results), but these are unrelated to mutation analysis, in any case.  The evaluation only considered reentrancy faults \cite{SurveyAttacks,FC20} (which are sometimes, but only rarely, introduced by mutants).  For reentrancy, Slither performed best on two real-world large contracts, finding subtle bugs in both, SmartCheck detected the problem in one of the two, and Securify detected neither.  For a dataset of 1,000 contracts, SmartCheck had a high false positive rate (over 70\%) but detected more actual reentrancies (209) than Slither (99) or Securify (6).  On the other hand, Slither's low false positive rate of 11\%  makes its results possibly more useful in practice.  It seems safe to say that 

For {\bf RQ4}, on the changes seen when restricting analysis to clean contracts, Slither did slightly better at detecting mutants when the original contract was clean for Slither, and the other two tools did somewhat worse on contracts for which they reported no findings.  For the three contracts clean for all tools, Slither performed almost exactly as it did over contracts in general, and the other tools performed worse, by about the same margin as they did for their own clean contracts.  For our approach, we only need a week version of the ``clean program assumption'':  the threat is that kills may be under-reported for non-clean programs, due to interference with findings for the original code.  It is not a problem if mutation scores are \emph{worse} for programs where a tool reports no findings for the un-mutated code.  We therefore, for smart contracts, find no threat to our approach arising from the presence of findings on un-mutated code.  We speculate that ``clean'' results for some tools result from contracts where the tool has trouble with the contract code, but does not actually crash; Slither may do better on clean code because it has fewer such failures, and clean contracts are probably generally simpler and easier to analyze.

\subsubsection{Improving Slither {\bf (RQ5 and RQ6)}}


\begin{figure}
  {\scriptsize
\noindent \begin{code}
 0x598ab825d607ace3b00d8714c0a141c7ae2e6822\_Vault.mutant.275.sol:
 if (!p.recipient.send(p.amount)) \{  // Make the payment
 ==>          if (true) \{  // Make the payment
 if (true) \{  // Make the payment
      \end{code}
      }
      \caption{Mutant showing {\bf Boolean constant misuse}.}
      \label{fig:boolean}
    \end{figure}

    \begin{figure}
      {\scriptsize
\begin{code}
 0x968815CD73647C3af02a740a2438D6f8219e7534\_TTPresale.mutant.311.sol:
 require(nextDiscountTTMTokenId6 >= 361 \&\& nextDiscountTTMTokenId6 <= 391);
 ==>  ...361...==>...0...
 require(nextDiscountTTMTokenId6 >= 0 \&\& nextDiscountTTMTokenId6 <= 391);
      \end{code}
      }
      \caption{Mutant showing {\bf Type-based tautologies}.}
      \label{fig:tautology}      
\end{figure}

\begin{figure}
  {\scriptsize
\begin{code}
 0x534ccee849a688581d1b0c65e7ff317ed10c5ed3\_NametagToken.mutant.480.sol:
 byte char = byte(bytes32(uint(x) * 2 ** (8 * j)));
 ==>  ...*...==>.../...
 byte char = byte(bytes32(uint(x) * 2 ** (8 / j)));
      \end{code}
      }
      \caption{Mutant showing {\bf Loss of precision}.}
      \label{fig:divmul}            
      \end{figure}


Based on the differential mutation analysis, we identified three low-hanging fruit to improve the performance of Slither.  The process was simple.  First, we produced a list of all mutants killed by either SmartCheck or Securify, but not killed by Slither.  We then applied the prioritization method based on the FPF algorithm and the distance metric described in Section \ref{sec:prioritizing}, and examined the mutants in rank order.  Many of the mutants were difficult to identify as true or false positives, absent context.  Some opportunities for enhancement were clear, but seemed likely to require considerable effort to implement without producing a large number of false positives.  For example, Securify often detected when an ERC20 token contract's guard preventing making the special 0x0 address the owner of a contract was removed, and issued the error {\tt Violation for MissingInputValidation}. Detecting such missing guards is probably useful, but formulating a way to do it without producing false positives is non-trivial.  We wanted to show that mutants could identify \emph{useful} but \emph{easy to implement} missing detectors.  Examining the first few mutants, we identified three such, based on mutants killed by either SmartCheck or Securify, or both:

\begin{enumerate}
\item {\bf Boolean constant misuse:}  This detector flags code like {\tt if (true)} or {\tt g(b || true)} (where {\tt g} is a function that takes a Boolean input).  Constant-valued conditionals tend to indicate debugging efforts that have persisted into production code, or other faults; there are almost no circumstances where a conditional should not vary with state or input.  This detector is actually split into two detectors, one for this serious issue, and an informational/stylistic detector that notes that code such as {\tt if (x == true)}, while semantically harmless, is difficult to read.  This problem was easily identified from cases such as the mutant in Figure \ref{fig:boolean}, killed by SmartCheck but not Slither:

\item {\bf Type-based tautologies:}  A type-based tautology is again a case where a Boolean expression has a constant value, but this is not due to misuse of a Boolean constant, but is instead due to the \emph{types} in a comparison.  For example, if {\tt x} is an unsigned integer type, the comparison {\tt x >= 0} is always true and {\tt x < 0} is always false.  This detector is a generalization of the SmartCheck detector \url{https://github.com/smartdec/smartcheck/blob/master/rule\_descriptions/SOLIDITY\_UINT\_CANT\_BE\_NEGATIVE/description\_en.html}, modified to actually compute the ranges of types and identify more general instances of tautological comparisons, e.g. {\tt y < 512} where {\tt y}'s type is {\tt int8}.  Again, the problem was easily identified by mutants killed by SmartCheck but not Slither such as the one in Figure \ref{fig:tautology}.

\item {\bf Loss of precision:}  Solidity only supports integer types.  This means that performing division before multiplication can introduce rounding that is not present when the multiplication is performed first.  This is a fairly important problem, given the frequency with which Solidity code performs financial calculations where maximum precision is desired.  SmartCheck provides a detector for such precision losses \url{https://github.com/smartdec/smartcheck/blob/master/rule\_descriptions/SOLIDITY\_DIV\_MUL/description\_en.html}, which enabled it to detect mutants such as the one shown in Figure\ref{fig:divmul}.
\end{enumerate}      

All three of these detectors were submitted as PRs, vetted over an internal benchmark set of contracts used by the Slither developers to evaluate new detectors, and accepted for release in the public version of Slither.  All three detectors produce some true positives (actual problems, though not always exploitable) in benchmark contracts, have acceptably low false positive rates, and were deemed valuable enough to include as non-informational (medium severity) detectors.  The first mutants in prioritized rank exhibiting the issues, shown above, were the 2nd, 9th, and 12th non-statement-deletion mutants ranked for SmartCheck, out of over 800 such mutants.  Using our prioritization, it was possible to identify these issues by examining fewer than 20 unkilled mutants.  Without prioritization, on average a developer would have to look at more than 200, 80, and 400 mutants, respectively, to find instances of these problems.  Interestingly, the very fact that these instances are ``needles in a haystack'' among the mutants not killed by Slither means that the results in Figure \ref{fig:solidityvenn} and Tables \ref{tab:scoresolidity} and \ref{tab:cleansolidity} are almost unaltered by our improvements to Slither: our analysis is fairly robust to modest tool improvements, unless added detectors account for a large number of mutants not detected by the tool.  Adding such detectors will also only improve mutation ratio if they do not add many false positives.  Substantial changes in results therefore require adding very effective (for mutants) detectors that seldom trigger for correct code (or at least trigger much less than for mutants).  ``Cheating'' with respect to a mutation benchmark is thus, we hope, difficult without actual major tool improvements.

There were 92 separately ranked statement deletion mutants also.  These, however, could all be ignored, as they were almost entirely duplicates related to the missing-return statement detector.  If this detector were not already present as a private Slither detector, it would also be a good candidate for addition to the tool.  Our three submitted detectors were not present as private detectors, and only one (the type-based tautology detector) had even been identified, via a GitHub issue, as a potential improvement (and only in the private version of Slither).  

Examining the first 100 mutants in the unprioritized lists for SmartCheck and Securify, ordered by contract ID and mutant number (roughly source line mutated) we were unable to identify \emph{any} obviously interesting mutants, suggesting that for {\bf RQ6} it is indeed hard to use mutation analysis results without prioritization. A large majority of the mutants we inspected involved either the missing
{\tt return} problem noted in the introduction, or replacing {\tt msg.sender} with {\tt tx.origin}; Slither, of course, has a detector for misuses of {\tt tx.origin}, and we believe (but are not sure) that almost all of the differences with respect to such changes are due to intentional behavior. SmartCheck and Securify tend to identify most (though not all) uses of {\tt tx.origin} as incorrect, while Slither has a more selective rule, intended to reduce false positives.
It is hard to scale our efforts here to a larger experiment, since writing and submitting changes to static analysis tools is always going to be a fairly onerous task, but we believe that our successful addition of new detectors, and the ease of identifying good candidate detectors using mutant prioritization supports a limited affirmative answer to {\bf RQ5} and {\bf RQ6}.

\subsection{Java Tools}

\subsubsection{Static Analysis Tools Compared}

For Java, we again compared three tools.  SpotBugs (\url{https://spotbugs.github.io/}) is the ``spiritual successor'' of FindBugs \cite{FindBugs,CompareJavaTools}.  PMD (\url{https://pmd.github.io/}) \cite{CompareJavaTools} is an extensible cross-language static code analyzer.  FaceBook's Infer (\url{https://fbinfer.com/}) \cite{Infer} focuses on diff-related detection of serious errors (concurrency, memory safety, and information flow) and aims to scale to very large code bases.

\subsubsection{Project Selection}

For Java and Python, we did not have to worry about invalidating tool improvements by basing our results on benchmark code.  We therefore aimed to use realistic, important source code.  We selected top GitHub projects (defined by number of stars) for each language, and removed projects with fewer than 5 developers or less than six months of commit history (as well as projects that did not build).  For Java, we analyzed the top 15 projects satisfying our criteria, with a maximum of 623,355 LOC and a minimum of 3,957 LOC, and a total size of 1.8 million LOC.  The Java compiler rejected a large number of the generated mutants, leaving us with just over 70,000 mutants to analyze, ranging from 136 to 10,016 per project.

\subsubsection{Analysis Results}


\begin{figure}
  \includegraphics[width=0.8\columnwidth]{java.png}
  \caption{Mutants killed by Java static analysis tools.}
  \label{fig:javavenn}
\end{figure}

\begin{table}
  \begin{tabular}{l|r|r|r|r|r}
    & \multicolumn{2}{|c|}{Findings} & \multicolumn{2}{|c|}{Mutation Score}  & Mutant \\
    Tool & Mean & Median & Mean & Median & Ratio\\
    \hline
    \hline
    SpotBugs & 0.47 & 0.00 & 0.37 & 0.21 & 0.7847 \\
    \hline
    PMD & 1.27 & 0.00 & 0.07 & 0.07 & 0.0562 \\
    \hline
    Infer & 0.07 & 0.00 & 0.00 & 0.00 &  0.0002 \\
    \hline
  \end{tabular}
  \caption{Java tools: number of findings and mutation scores over all projects.}
  \label{tab:scorejava}
\end{table}

\begin{table}
  \begin{tabular}{l|r|r|r}
    & & \multicolumn{2}{|c|}{} \\
    & \# Clean & \multicolumn{2}{|c|}{Mutation Score} \\
    Tool & Projects & Mean & Median \\
    \hline
    \hline
    SpotBugs & 3 & 0.41 & 0.17 \\
    \hline
    PMD & 0 & N/A & N/A \\
    \hline
    Infer & 6 & 0.00 & 0.00 \\
    \hline
  \end{tabular}
  \caption{Java tools: clean project counts and mutation scores.  No projects were clean for all tools, due to PMD producing findings for all projects.}
  \label{tab:cleanjava}
\end{table}

Figure \ref{fig:javavenn} shows the mutants killed by the Java analysis tools, and Tables \ref{tab:scorejava} and \ref{tab:cleanjava} provide numeric results for projects and clean projects, respectively.

In terms of {\bf RQ1}, the raw results are clear:  SpotBugs finds vastly more uniquely killed mutants than the other tools, but PMD contributes a large number of its own unique detections, and clearly offers some independent utility.  Infer on the other hand, is only able to detect two mutants, but these are unique.  Unfortunately, these mutants only served to indicate that we were actually running Infer in such a way that it \emph{could} detect problems:  both were concurrency warnings, but the code change in both cases was simply removing a (semantically inoperative) {\tt \@Override} annotation.  It may be that the diff sizes in our code were simply too small for Infer's approach.

{\bf RQ2} is also answered in the affirmative.  While the raw kills for SpotBugs are impressive, it also produced fewer findings for clean code, giving it an excellent mutant ratio, making its dominance even more clear.  PMD's performance was acceptable, and better than any of the Solidity tools (note that comparing ratios across languages is potentially invalid, but not obviously so).  Infer is confirmed as nearly useless, again.

\begin{figure}
  {\scriptsize
  \begin{itemize}
  \item \url{https://stackoverflow.com/questions/4297014/what-are-the-differences-between-pmd-and-findbugs}
  \item \url{https://www.sw-engineering-candies.com/blog-1/comparison-of-findbugs-pmd-and-checkstyle}
  \item \url{https://www.reddit.com/r/java/comments/3i7w6n/checkstyle_vs_pmd_vs_findbugs_for_dummies_why/}
  \end{itemize}
  }
\caption{Blog posts and forum threads discussing Java static analysis tools.}
\label{fig:blog}
\end{figure}

For {\bf RQ3}, to our knowledge there is no recent academic comparison of these tools; the most relevant study dates from 2004 \cite{CompareJavaTools}, used FindBugs, not SpotBugs, and reached no strong conclusions with respect to FindBugs vs. PMD.  However, the user postings listed in Figure \ref{fig:blog}, plus personal communications with security analysts who use these tools (citation removed for now, due to potential to compromise blinding) supported some basic conclusions.  SpotBugs/FindBugs is indeed the best tool for finding bugs; PMD focuses more on stylistic issues and has a weaker semantic model.  Mutants are much more likely to be bugs than stylistic issues (they alter semantics, not code style, in almost all cases: deletion or constant/operator changes obviously have no style impact).  Infer is closer to a model checker focusing on resource leaks than a truly general-purpose tool, arguably.  In fact, we suspect Infer would perform much better if we crafted complex mutation operators targeting some important subtle Java bugs; it is still fair to say that Infer is probably not a good \emph{general-purpose} static analysis tool for Java.

For {\bf RQ4} there were very few clean projects, but the mutation scores for Infer were unchanged, and for SpotBugs the mean mutation score improved and median mutation score was worse.  Again, we see no evidence that non-clean code is a source of significant degradation on mutation detection.


\subsection{Python Tools}

\subsubsection{Static Analysis Tools Compared}

We compared three widely used and well-known Python tools:  Pylint \url{https://www.pylint.org/} (probably the most widely used of Python bug finding tools), pyflakes \url{https://pypi.org/project/pyflakes/}, designed to be faster, lighter-weight, and more focused on bugs (without configuration) than Pylint, and PyChecker \url{http://pychecker.sourceforge.net/}, an older, but still used (e.g., we found numerous {\tt .travis.yml} files installing and running PyChecker), tool.

\subsubsection{Project Selection}

For Python, we analyzed the top 25 GitHub projects by our criteria (see above), due to the smaller size of Python projects.  These ranged in size from 3,377 LOC to 34,671 LOC, with a total size of just under 100 KLOC.  For Python, we were able to produce 167,511 valid mutants to analyze.

\subsubsection{Analysis Results}


\begin{figure}
  \includegraphics[width=0.8\columnwidth]{python.png}
  \caption{Mutants killed by Python static analysis tools.}
  \label{fig:pythonvenn}
\end{figure}

\begin{table}
  \begin{tabular}{l|r|r|r|r|r}
    & \multicolumn{2}{|c|}{Findings} & \multicolumn{2}{|c|}{Mutation Score}  & Mutant \\
    Tool & Mean & Median & Mean & Median & Ratio\\
    \hline
    \hline
    Pylint & 2.24 & 1.00 & 0.22 & 0.20 & 0.097 \\
    \hline
    pyflakes & 2.4 & 1.00 & 0.03 & 0.00 & 0.012 \\
    \hline
    PyChecker & 0.56& 0.00 & 0.10 & 0.00 &  0.183 \\
    \hline
  \end{tabular}
  \caption{Python tools: number of findings and mutation scores over all projects.}
  \label{tab:scorepython}
\end{table}

\begin{table}
  \begin{tabular}{l|r|r|r}
    & & \multicolumn{2}{|c|}{} \\
    & \# Clean & \multicolumn{2}{|c|}{Mutation Score} \\
    Tool & Projects & Mean & Median \\
    \hline
    \hline
    Pylint & 0 & N/A & N/A \\
    \hline
    pyflakes & 12 & 0.02 & 0.00 \\
    \hline
    PyChecker & 21& 0.01 & 0.00 \\
    \hline
  \end{tabular}
  \caption{Python tools: clean project counts and mutation scores.  No projects were clean for all tools, due to Pylint producing findings for all projects.}
  \label{tab:cleanpython}
\end{table}


Figure \ref{fig:pythonvenn} shows the mutants killed by the Python analysis tools, and Tables \ref{tab:scorepython} and \ref{tab:cleanpython} provide numeric results for projects and clean projects, respectively.

For {\bf RQ1}, Pylint and pyflakes both killed more than 15,000 mutants killed by no other tool; Pylint killed more mutants, but the relative difference was much smaller than in our Solidity or Java analysis.  It is clear both tools are well worth using, assuming these are not false positives.  Pyflakes' mutation score is lower than unique kills might suggest because it has almost no overlap with the other two tools.  It is doing something quite different than they are, apparently.

The mutant ratios {\bf RQ2} suggest that PyChecker might be better than pyflakes, in fact, and better than Pylint, because while it killed fewer mutants, it also produced fewer findings by far, overall.  PyChecker would be a good choice for quick and dirty analysis of code on the fly, perhaps, in that it had excellent mutation scores for some programs (hence a high mean) but is relatively ``quiet'' compared to the other tools.  However, its very low median mutation score is troubling; it may simply not work for some code.  A combination of pyflakes and Pylint seems safer, though adding the relatively low-findings PyChecker seems likely to have little cost.  For Python, we see that all of our measures are required to get a good picture of the tools.

\begin{figure}
  {\scriptsize
  \begin{itemize}
  \item \url{https://stackoverflow.com/questions/1428872/pylint-pychecker-or-pyflakes}
  \item \url{https://www.reddit.com/r/Python/comments/ii3gm/experience_with_pylint_pychecker_pyflakes/}
  \item \url{https://www.slant.co/versus/12630/12631/~pylint_vs_pyflakes}
  \item \url{https://news.ycombinator.com/item?id=12748885}
  \item \url{https://doughellmann.com/blog/2008/03/01/static-code-analizers-for-python/}
  \end{itemize}
  }
\caption{Blog posts and forum threads discussing Python static analysis tools.}
\label{fig:blogpython}
\end{figure}

For {\bf RQ3}, there were again no academic comparisons we could find.  However, opinions on the web were quite common (see Figure \ref{fig:blogpython}, which lists ones we examined).  It is hard to summarize the overall opinion here, since it ranges considerably.  There is probably general agreement that PyChecker is old and maybe less useful, but also terse and sometimes helpful.  Pylint is the most recommended tool, and the general complaint that it is too picky was mitigated in our results by turning off clearly informative-only findings; users not configuring Pylint will probably see even more killed mutants, but likely a worse mutant ratio because mutants tend to introduce non-stylistic problems.  Pyflakes is also well liked, and supposedly less verbose than Pylint; we did not see this in our results, but that may be because we configured Pylint, and pyflakes aims to focus on ``actual bugs'' so it is equally verbose for mutants, which are likely to be actual bugs.

For {\bf RQ4} there were no clean projects for the known-to-be-aggressive Pylint.  Performance for the other two tools was worse on projects for which they were clean, again failing to support the possibility that our definition of killing a mutant under-reports detection for non-clean code.

\subsection{Threats to Validity}

The primary threat to validity in terms of generalization is that we only examined nine static analysis tools, and our analyses were restricted to 100 smart contracts, 15 Java projects, and 25 Python projects.   Because it is hard to identify a ground truth to compare with (the motivation for our approach), we cannot be certain that our rankings of tools are correct even for these tools and this code.  However, where there are existing discussions of the tools, our results seem to agree with these, but add substantial detail.

We used the Universal Mutator \cite{universalmutator,regexpMut}, which aggressively produces large numbers of mutants, but does not target any particular software defect patterns, to generate all mutants.  We will make available a repository containing our full raw results plus details on contracts and projects selected, for replication and further analysis, upon acceptance (to avoid breaking blinding).
