We thank the reviewers for their helpful comments. 

R1: 

Most importantly: https://github.com/mutantsforstaticanalysis/rawdata has the raw data. We hope this helps! The full set of mutation operators can be easily extracted by examining the rule files in the universalmutator github repo. 

With respect to the meaningfulness of the mutants, first note that we do not examine mutants detected by zero tools, due to the differential nature of the approach; these are likely noise/uninteresting/not plausibly statically detected, but we just ignore them. Only when at least one tool detects a mutant do we care about it (and mostly when at least one does not). 

Qualitative analysis is hard to make completely scientific, but we agree the paper omitted the forest for the trees, to make way for details of tool comparisons. We did in fact inspect a large number of the mutants in the "diff" (via random sampling for Java/Python). For Solidity, where we manually examined the majority of the mutants killed by at least one tool, _most appeared to be semantically "troubling" at minimum_, or clearly faults, though we think many changes of msg.sender to tx.origin flagged by non-Slither tools may be valid (if strange) code. For Java, a large number of mutants detected by one tool but not another involved deleted function calls or conditional changes (e.g. == null to != null) that introduce potential null pointer exceptions. These show differences in analysis methods rather than missing detectors, and are clearly faulty code that should be detected. For Python, mutants producing code that would cause runtime (but not parsing) failures were common, often just an access to a now-undefined variable or obvious type mismatch. Again, all tools aim to detect these, so it is engine more than detector differences. Other Java/Python cases were harder to understand, but many seemed to be real, statically detectable faults. We will add this type of qualitative analysis. 

Yes, we actually used "number" of warnings, as a conservative measure of detection. If a mutant only causes a tool to change finding message, then we do not consider that a kill. If it loses one warning and produces another then it is not a kill. The core reason we don't consider the location/nature of the warning change is that in some cases a mutant should cause a warning about code far from the mutant location. Therefore, we strictly require that a tool produce _more warnings_ when code is mutated, in order to be counted as detecting the mutant; we assume each mutant of interest potentially adds a fault, and fixes no existing problems. We experimented with other definitions on a set of smart contracts, and this approach produced the most useful results. It is thus possible (though we didn't see this happen with smart contracts) that we undercount cases where the original code produces warning A and the mutated code "fixes" problem A but introduces problem B. This seems like an acceptable risk to take, given mutants seldom fix faults, and better than having to parse/understand warning types/content to apply the method to every new language/tool. 

Good point, the source/bytecode choice is language dependent. As to why we chose Slither, see below. 

We do observe project level differences, and will report them. 

While not for Java, universalmutator has been extensively tuned for Python. 

We’ll add those references! 

R2: 

We chose PMD primarily because we started by focusing on [43] as a comparison paper, rather than [28], due to the highly-focused scope of [28] (only Defects4J bugs, thus many many fewer relevant warnings/aspects of code considered, and any biases in producing the Defects4J data set). Adding Error Prone would be good, and should be possible by camera-ready. Arguably, [43] is the closer comparison to our work: larger scope of interest, albeit with higher risk of invalid results over that scope. 

"Importing" rules from one tool to another can be very helpful for users. If one tool uses a better underlying analysis engine (e.g., better intermediate language, better alias/data-flow/etc. approach), then adding a rule from another tool can result in much better _actual_ detection of bugs, or fewer false positives. Slither's IR and analysis are probably more scalable than Securify and more precise than SmartCheck, so even just using the same rules gives everyone better analysis results. Also, the rules are generalizations that expand on the rules in SmartCheck/Securify: using mutations makes generalizing a specific rule somewhat easier, we believe, due to (often multiple) concrete examples, not just a high-level concept. 

We primarily used universalmutator because PIT is a bytecode-level mutation tool only. PMD requires source code, and we believe users want to inspect some mutants. We needed to manually do validation, as R1 points out, which is hard for bytecode. We wanted to use a consistent set of operators and mutation approach, and universalmutator was the best-supported tool for the other languages. 

In RQ4 "better" means _higher_ mutation scores were hypothesized for programs for which there were no findings in the original code. 

Securify is a static analysis tool, but it uses souffle under the hood to solve Prolog constraints as part of building a dependency graph; this can scale very poorly. 

Our reasoning on the mutant ratio for Securify is that Securify does not notably produce more warnings for mutants (which are usually bugs in some sense, even if not statically detectable) compared to the vast number of warnings it issues in general. Other research supports the claim that Securify is prone to false positives, and its behavior on mutants is not appreciably different than on other code, while SmartCheck and Slither have 20x better ratios. 

R3: 

Yes, we had to understand each tool's finding types (for Pylint this was painful) and filter out ones that are essentially stylistic rather than "here’s a bug!". We made considerable effort to ensure these choices were fair to the tools. Some tools made it easy by simply providing a setting to turn off "informational and style" issues, but for others it is a (minor, we think) threat to validity.