We would like to thank the anonymous reviewers for their helpful comments.

R1:

Most importantly: https://github.com/mutantsforstaticanalysis/data has the raw data.  We hope this helps!  The full set of mutation operators can be easily extracted by examining the rule files in universalmutator github repo. 

With respect to the meaningfulness of the mutants, first note that we do not examine mutants detected by zero tools, due to the differential nature of the approach; these are likely noise/uninteresting/not plausibly statically detected, but we just ignore them.  Only when at least one tool detects a mutant do we care.

Qualitative analysis is hard to make completely scientific, but we agree the paper omitted the forest for the trees, to make way for details of tool comparisons.  We did in fact inspect a large number of the mutants in the "diff" (via random sampling for Java/Python).  For Solidity, where we manually examined the majority of the mutants killed by at least one tool, _most appeared to be semantically "troubling" at minimum_, or clearly faults, though we think many changes of msg.sender to tx.origin flagged by non-Slither tools may be valid (if strange) code.  For Java and Python, many mutants detected by one tool but not another involved deleted function calls or conditional changes (e.g. == null to != null) that would clearly introduce potential null pointer exceptions or method calls on None.  These show differences in analysis methods rather than missing rules, but are clearly real(istic) faults.  Other Java/Python cases were harder to understand, but many seemed to be real, statically detectable faults (e.g., introducing possible division by zero).

Yes, we actually used "number" of warnings, as a conservative measure of detection.  If a mutant simply causes a tool to change the message it reports, then we do not consider that a kill.  Similarly, if it loses one warning and produces another (which is in a sense the same thing, from a user's perspective), then it is not a kill.  The core reason we don't consider the location/nature of the warning change is that in some cases a mutant causes a warning about code far from the mutant location.  Therefore, we strictly require that a tool produce _more warnings_ when code is mutated, in order to be counted as detecting the mutant.  We experimented with other definitions on a small set of smart contracts, and this was much better than other approaches, in terms of the results making sense.  It is possible (though we didn't see this in practice with smart contracts) that it undercounts cases where 1) the original code produces warning A and 2) the mutated code "fixes" problem A and causes detection of a different problem, B.  If the mutant doesn't change the issue, just changes the exact nature of the problem, then we argue the mutant didn't introduce a fault, it just changed some aspect of an existing fault, which is not what we are interested in.  If we want to compare tools over existing faults, that is already doable.

Good point, the source/bytecode choice is language dependent.  As to why we chose slither, see below.

R2:

We chose PMD primarily because we started by focusing on [43] as a comparison paper, rather than [28], due to the highly-focused scope of [28] (only Defects4J bugs, thus many many fewer relevant warnings/aspects of code considered, and any biases in producing the Defects4J data set).  Adding Error Prone as well would be a good idea.  In a way, [43] is a closer comparison to our work:  larger scope of interest, if higher risk of invalid results over that scope.

"Importing" rules from one tool to another can be very helpful for actual users.  If one tool uses a better underlying analysis engine (e.g., better intermediate language, better alias/data-flow/etc. approach), then adding a rule from another tool can result in much better _actual_ detection of bugs, or fewer false positives.  Slither's IR and analysis are probably more scalable than Securify and more precise than SmartCheck, so even just using the same rule gives everyone better analysis results.  Also, the rules are generalizations that expand on the rules in SmartCheck/Securify: the mutation approach makes generalizing a specific rule somewhat easier, we think, due to providing multiple examples, not just a high-level concept.

We used universalmutator for two reasons:  primarily because PIT is a bytecode-level mutation tool only, PMD requires source code, and we believe users want to inspect mutants for this application (and we needed to manually do validation as R1 points out, which is hard for bytecode).  We also wanted to use a consistent set of operators and mutation approach, and universalmutator was the best tool for the other languages.

In RQ4 "better" means _higher_ mutation scores were hypothesized for programs for which there were no findings in the original code.

Securify is a static analysis tool, but it "symbolically analyzes the contract’s dependency graph to extract precise semantic information from the code" before checking compliance/violation patterns.  The first step, unfortunately, can sometimes scale abominably on real code.

Our reasoning on the mutant ratio for Securify is that Securify does not notably produce warnings for mutants (which are probably bugs in some sense, even if not statically detectable) compared to the vast number of warnings it issues in general.  Other research supports the claim that Securify is prone to false positives over all code, and its behavior on mutants is not appreciably different (see the 20x or more higher ratios for the other tools.

R3:

Yes, we had to understand each tool's finding types (for Pylint this was quite painful) and filter out ones that are essentially stylistic rather than "here’s a bug!".  We made considerable effort to ensure these choices were fair to the tools.  Some tools made it easy by simply providing a setting to turn off "informational and style" issues, but for others it is a (minor, we think) threat to validity.

