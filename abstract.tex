Many programming languages offer multiple static analysis tools that offer to detect faults in code without executing it.  Understanding the strengths and weaknesses of tools, and performing direct comparisons of their effectiveness is difficult; it usually involves either manual examination of differing warnings on real code, or the bias-prone construction of artificial test cases.  In practice, comparisons tend to be limited to superficial, anecdotal discussions in the informal literature (e.g., blog posts by software developers), or purely research-community-oriented evaluations made by the authors of new tools seeking to publish their results.  This paper proposes a novel automated approach to comparing static analysis tools, based on producing \emph{mutants} of real code, and comparing mutation detection rates for tools to their warning rates on the original code.  In addition to making tool differences quantitatively observable without extensive manual effort, this approach offers a new way to detect and fix omissions in a static analysis tool's set of detectors.  We present an extensive comparison of three well-known Solidity smart contract static analysis tools, and show how using an automatic prioritization of our results allowed us to add three effective new detectors as an open source contribution to the best of the tools.  We also evaluate popular Java and Python static analysis tools and discuss their strengths and weaknesses.