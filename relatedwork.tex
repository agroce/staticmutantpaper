\section{Related Work}

The goal of ``analysing the program analyser'' \cite{cadar2016analysing} and applying better automated methods to evaluate and improve analysis tools has become recently more popular and, we suspect, more possible.  The irony of using mostly ad-hoc, manual methods to test and understand static analysis tools is apparent; however, the fundamentally incomplete and heuristic nature of effective analysis tools makes this a challenge similar to testing machine learning algorithms \cite{OnlyOracle}; most tools will not produce ``the right answer'' all the time, by their very nature.  This is a result of both algorithmic constraints and basic engineering trade-offs.  While comparisons of static analysis tools \cite{CompareJavaTools,durieux2019empirical, Parizi,slither} have appeared in the literature for years, these generally involved large human effort and resulting smaller scale, or did not make a strong effort to address false positives, or restricted analysis to, e.g., a known defects set \cite{AllBugs}.

Cuoq et al. \cite{regehrRandom} proposed the generation of random programs (\emph{\'a la} Csmith \cite{csmith}) to test analysis tools aiming for soundness, in limited circumstances, but noted that na\"ive differential testing of analysis tools was not possible.  This paper proposes a non-na\"ive differential comparison (not, exactly, differential testing, however, in that only aggregate results are possible to interpret without human intelligence), based on the observation that the ability to detect program mutants offers an automatable way to tell which of two tools is better (for a given universe of examples, at least) at telling faulty from non-faulty code.

Klinger et al. propose a different approach to differential testing of analysis tools \cite{klinger2019differentially}.  Their approach is in some ways similar to ours, in that it takes as input a set of seed programs, and compares results across new versions generated from that seed.  The primary differences are that their seed programs must be warning-free (which greatly limits the set of input programs available) and their tool must parse and understand the programs, and that the new versions are based on adding new assertions, not ``breaking'' the original code.  We allow arbitrarily buggy seed programs (thus many more real programs can be used), and can, due to the any-language nature of the mutation generator we use, operate even in new languages without further development effort.  Further, their approach only identifies problems when tools are outliers compared to numerous other tools in either detecting a bug (precision) or not detecting it (soundness), and so requires comparing multiple tools.  Our approach has some utility for even a single tool (you can just examine prioritized un-detected mutants).  On the other hand, their approach can identify precision issues, while we offer no real help with false positives (in theory, you could apply their majority-vote method to mutants only a few tools flag, but mutants \emph{are} usually faults. in contrast to their introduction of checks that may be guaranteed to pass, so this is probably not very helpful).  Most importantly, however, their approach only applies to tools that check assertions, rather than more general, popular static analysis tools that only identify bad code patterns.

Finally, the large body of work on using mutants in software testing \cite{jia2011analysis,demillo1978hints,budd1980theoretical, groce2015verified,groce2018verified,MutGoogle,ivankovic2018industrial,mutKernel} is obviously relevant, in that we basically adopt its approach, but re-define killing a mutation for a static analysis context.