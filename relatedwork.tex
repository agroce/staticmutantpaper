\section{Related Work}

The goal of ``analysing the program analyser'' \cite{cadar2016analysing} is intuitively attractive.  The irony of using mostly ad-hoc, manual methods to test and understand static analysis tools is apparent; however, the fundamentally incomplete and heuristic nature of such tools makes this a challenge similar to testing machine learning algorithms \cite{OnlyOracle}; most tools will not produce ``the right answer'' all the time, as a result of both algorithmic constraints and basic engineering trade-offs.  While comparisons of static analysis tools \cite{CompareJavaTools,durieux2019empirical, Parizi,slither,pashchenko2017delta} have appeared in the literature for years, these generally involved large human effort and resulting smaller scale, did not make a strong effort to address false positives, or restricted analysis to, e.g., a known defects set \cite{AllBugs, do2016toward}.  Defect sets are vulnerable to tools intentionally overfitting/gaming the benchmark; our approach makes it easy to compare tools on ``fresh'' code to avoid this risk.  Compared to well-known studies of Java tools \cite{AllBugs,CompareJavaTools} our approach used a larger set of subject programs (1.8 MLOC total vs. 170-350KLOC) and thousands of \emph{detected} faults, vs. e.g., about 500 known defects \cite{AllBugs}.  Results not using defect sets are even more limited in that humans can only realistically examine a few dozens of each type of warning \cite{CompareJavaTools}.

Cuoq et al. \cite{regehrRandom} proposed the generation of random programs (\emph{\'a la} Csmith \cite{csmith}) to test analysis tools aiming for soundness, in limited circumstances, but noted that na\"ive differential testing of analysis tools was not possible.  This paper proposes a non-na\"ive differential comparison based on the observation that the ability to detect program mutants offers an automatable comparison basis.  We essentially adopt the approach of the large body of work on using mutants in software testing \cite{jia2011analysis,demillo1978hints,budd1980theoretical, groce2015verified,groce2018verified,MutGoogle,ivankovic2018industrial,mutKernel}, but re-define killing a mutation for a static analysis context.

Klinger et al. propose a different approach to differential testing of analysis tools \cite{klinger2019differentially}.  Their approach is in some ways similar to ours, in that it takes as input a set of seed programs, and compares results across new versions generated from seeds.  The primary differences are that their seed programs must be warning-free (which greatly limits the set of input programs available), and that the new versions are based on adding new assertions only.  We allow arbitrarily buggy seed programs, and can, due to the any-language nature of the mutation generator we use, operate even in new languages.  On the other hand, their approach can identify precision issues, while we offer no real help with false positives (in theory, you could apply their majority-vote method to mutants only a few tools flag, but mutants \emph{are} usually faults. in contrast to their introduction of checks that may be guaranteed to pass).  Most importantly, however, their approach \emph{only applies to tools that check assertions}, rather than the more common case of tools that identify bad code patterns (e.g. all tools we considered in this paper).
